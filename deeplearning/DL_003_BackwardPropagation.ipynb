{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c57c96b",
   "metadata": {},
   "source": [
    "# How to do Backpropagation with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82edeabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bb4a5",
   "metadata": {},
   "source": [
    "## Layer class\n",
    "\n",
    "Like they provide a *forward* function, all our different layer types will get an *backward* function as well. Here we pass the upstream gradient as well as the original inputs to the function. Based on this the Layer will calculate the downstream gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    An identity Layer to be the base of inheritance\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # identity, same output as input\n",
    "        return input\n",
    "    \n",
    "    def backward(self, input, grad_output, verbose=False):\n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad910cb",
   "metadata": {},
   "source": [
    "## Updating the Sigmoid Function \n",
    "\n",
    "The derivative of the function\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\sigma(x) = \\frac{1}{1+e^{-x}}$ \n",
    "\n",
    "is \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\sigma'(x) = \\sigma(x) \\cdot (1-\\sigma(x))$\n",
    "\n",
    "The chain rule says:\n",
    "$\\frac{\\partial L}{\\partial input} = \\frac{\\partial L}{\\partial output} \\cdot \\frac{\\partial output}{\\partial input} = upstream gradient * local gradient$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29db5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x)*(1- sigmoid(x))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return sigmoid(input)\n",
    "    \n",
    "    def backward(self, input, grad_output, verbose=False):\n",
    "        # apply the cain rule\n",
    "        return grad_output * derivative_sigmoid(input) \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Sigmoid()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1130a6",
   "metadata": {},
   "source": [
    "Out input can be a vector or a matrix. Usually it will be a matrix of $batchsize \\times inputnodes$. Thus the gradients will be of size $batchsize \\times outputnodes$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62cbb5",
   "metadata": {},
   "source": [
    "# Implmenting a RELU Layer\n",
    "\n",
    "ReLU is similar to $max$ regarding backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Apply elementwise ReLU to [batch, input_units] matrix\n",
    "        return np.maximum(0,input)\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        # Compute gradient of loss w.r.t. ReLU input\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375d24c",
   "metadata": {},
   "source": [
    "# Backward function for the Dense Layer\n",
    "\n",
    "In the activation functions we just backpropagated gradients, we had no weights to update. This time we do both \n",
    "* first we calculate gradients with respect to inputs (weights fixed)\n",
    "* then we calculate the gradient with respect to weights (inputs fixed)\n",
    "* we calculate the gradient with respect to the bias values (inputs fixed)\n",
    "* Finally weights and biases are updated consigering gradients and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate):\n",
    "        # initialize weights with small random numbers. We use normal initialization, \n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.learning_rate=learning_rate\n",
    "                    \n",
    "    def forward(self,input):\n",
    "        return input.dot(self.weights)+self.biases;\n",
    "    \n",
    "    def backward(self,input,grad_output,verbose=False):\n",
    "        #compute downstream gradient = gradient with respect to inputs\n",
    "        grad_input = np.dot(grad_output, self.weights.T) \n",
    "        \n",
    "        #compute gradient with respect to weights and biases\n",
    "        grad_weights = np.dot(input.T,grad_output)/input.shape[0] \n",
    "        grad_biases = grad_output.mean(axis=0) \n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        \n",
    "        #now we can sounchronuously update the weights = one stochastic gradient descent step. \n",
    "        self.weights = self.weights - self.learning_rate*grad_weights\n",
    "        self.biases = self.biases - self.learning_rate*grad_biases\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def __str__(self):\n",
    "        text = f\"Dense Layer {self.weights.shape}\"\n",
    "        return text        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c89348",
   "metadata": {},
   "source": [
    "## Integrating a gradient descent step to the network\n",
    "\n",
    "Our first network shall just be able to learn the \"and\"-Function for binary values (0,1). So it can be rather small with just one layer so its easy to debug. Later we will increase the number of layers.\n",
    "\n",
    "* During forward we will store all the inputs/outputs between the layers\n",
    "* A dedicated predict function returns the outputs of the last layer\n",
    "* We have to implement the derivative of the loss function\n",
    "* We implement \"trainBatch\" which does one gradient descent step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72180229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, learning_rate): \n",
    "        self.layers = []\n",
    "        self.layers.append(Dense(2,1, learning_rate))\n",
    "        self.layers.append(Sigmoid())\n",
    "        print(\"Number of layers\", len(self.layers))\n",
    "        \n",
    "    def __str__(self):\n",
    "        text = \"Network:\\n\"\n",
    "        for lay in self.layers:\n",
    "            text = text + \"  \" +lay.__str__() + \"\\n\"\n",
    "        return text\n",
    "    \n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X);\n",
    "            activations.append(X)\n",
    "        assert len(activations) == len(self.layers)+1\n",
    "        #print(\"activations.shape\", activations.shape)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, input, verbose=False):\n",
    "        return self.forward(input)[-1]\n",
    " \n",
    "    def loss(self, y, pred):\n",
    "        assert y.shape == pred.shape\n",
    "        vec = -np.log(pred)*y-(np.log(1-pred)*(1-y))\n",
    "        return np.mean(vec);\n",
    "    \n",
    "    # https://stats.stackexchange.com/questions/428228/how-does-cross-entropy-log-loss-work-with-backpropagation\n",
    "    def log_loss_prime(self, y_true, y_pred):\n",
    "        return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
    "    \n",
    "    def trainBatch(self, X, y, verbose = False):\n",
    "        layer_inputs = self.forward(X)\n",
    "        pred = layer_inputs[-1];\n",
    "        if verbose: \n",
    "            print(\"layer_inputs\")\n",
    "            for lay in layer_inputs:\n",
    "                print(lay)\n",
    "            print(\"predictions\", pred)\n",
    "\n",
    "        # Compute the loss and the initial gradient\n",
    "        loss_grad=self.log_loss_prime(y,pred)\n",
    "        if verbose: print(\"Shape \", loss_grad.shape, \" Value:\", loss_grad)\n",
    "    \n",
    "        index = -2;\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(layer_inputs[index],loss_grad, verbose=verbose) #grad w.r.t. input, also weight updates\n",
    "            if (verbose):\n",
    "                print(\"Layer:\", index, layer);\n",
    "                print(\"Loss_grad:\", loss_grad);\n",
    "\n",
    "            index = index -1\n",
    "        \n",
    "        return self.loss(y, pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9798356",
   "metadata": {},
   "source": [
    "## Make a prediction with an untrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f06e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(0.1)\n",
    "input = np.array([\n",
    "    [1.0,1.0],\n",
    "    [1.0,0.0],\n",
    "    [0.0,1.0],\n",
    "    [0.0,0.0],\n",
    "]) \n",
    "y = np.array([1.0,0.0,0.0,0.0]).reshape(4,1)\n",
    "pred = net.predict(input)\n",
    "print(\"prediction\",pred)\n",
    "assert y.shape == pred.shape\n",
    "print(\"loss\",net.loss(y,pred))\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88189e",
   "metadata": {},
   "source": [
    "## Do gradient descent with multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.trainBatch(input,y)\n",
    "#print(net)\n",
    "for i in range(10):\n",
    "    for j in range(10000):\n",
    "        net.trainBatch(input,y)\n",
    "    print(net.trainBatch(input,y))\n",
    "    \n",
    "#print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b29dff",
   "metadata": {},
   "source": [
    "## Predict again with trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net.predict(input)\n",
    "print(\"prediction\",pred) \n",
    "assert y.shape == pred.shape\n",
    "print(\"loss\",net.loss(y,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c69ef",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95ac4b",
   "metadata": {},
   "source": [
    "* Implement a \"fit\" method to the Network class for training\n",
    "* Experiment with the learning-rate and iterations\n",
    "* Show that this model is still a linear model and plot the decision border\n",
    "* Train a model that does the AND-Function on floatvalues like in the forward path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
